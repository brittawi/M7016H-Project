{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptive_statistics import DiabetesDataBase\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from utils import grid_search, halving_random_search, validate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_method = \"Q\" # (\"S\", \"R\", \"Q\") S for standard, R for robust, Q for quantile\n",
    "match normalization_method:\n",
    "    case \"S\":\n",
    "        SCALER = StandardScaler()\n",
    "    case \"R\":\n",
    "        SCALER = RobustScaler() #Eliminated never better than S or Q and take \n",
    "    case \"Q\":\n",
    "        SCALER = QuantileTransformer(n_quantiles=334)\n",
    "\n",
    "RANDOM_STATE = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "time = datetime.now().strftime(\"%Y%B%d_%H_%M\")\n",
    "print(time)\n",
    "log_folder = \"logs/\"+time+normalization_method+str(RANDOM_STATE)\n",
    "writer = SummaryWriter(log_dir=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"diabetes.csv\"\n",
    "ddb = DiabetesDataBase(csv_path, train_split=0.8, val_split=0.1, test_split=0.1, random_state=RANDOM_STATE, augment=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = ddb.get_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {\n",
    "    \"classifier__n_neighbors\": list(range(1, 31)),\n",
    "    \"classifier__metric\": ['euclidean','manhattan'],\n",
    "    \"classifier__weights\":['uniform','distance']\n",
    "}\n",
    "\n",
    "knn_cls = grid_search(knn, SCALER, parameters)\n",
    "\n",
    "knn_cls.fit(X_train, y_train)\n",
    "\n",
    "print(knn_cls.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "knn_avg, knn_cm = validate(knn_cls, X_val, y_val)\n",
    "writer.add_scalars(\"knn\", knn_avg)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state = RANDOM_STATE)\n",
    "\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "parameters = {\n",
    "    \n",
    "    \"classifier__n_estimators\": [int(x) for x in np.linspace(start = 100, stop = 1000, num = 100)],\n",
    "    \"classifier__max_features\": ['log2', 'sqrt'],\n",
    "    \"classifier__max_depth\" : [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "    \"classifier__min_samples_split\": [2,5,10],\n",
    "    \"classifier__min_samples_leaf\": [1, 2, 4],\n",
    "    \"classifier__bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "random_forest_cls = halving_random_search(random_forest, SCALER, parameters, random_state=RANDOM_STATE)\n",
    "\n",
    "random_forest_cls.fit(X_train, y_train)\n",
    "\n",
    "print(random_forest_cls.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "random_forest_avg, random_forest_cm = validate(random_forest_cls, X_val, y_val)\n",
    "writer.add_scalars(\"random_forest\", random_forest_avg)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "\n",
    "parameters = {\n",
    "    \"classifier__penalty\": [None, \"l2\"]\n",
    "}\n",
    "\n",
    "reg_cls = grid_search(log_reg, SCALER, parameters)\n",
    "\n",
    "reg_cls.fit(X_train, y_train)\n",
    "\n",
    "print(reg_cls.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "log_reg_avg, log_reg_cm = validate(reg_cls, X_val, y_val)\n",
    "writer.add_scalars(\"log_reg\", log_reg_avg)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "parameters = {\n",
    "    \"classifier__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"classifier__gamma\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"classifier__kernel\": [\"linear\", \"rbf\", \"sigmoid\"]\n",
    "}\n",
    "print(parameters)\n",
    "\n",
    "\n",
    "svm_cls = grid_search(svm, SCALER, parameters)\n",
    "svm_cls.fit(X_train, y_train)\n",
    "print(svm_cls.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "svm_avg, svm_cm = validate(svm_cls, X_val, y_val)\n",
    "writer.add_scalars(\"svm\", svm_avg)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(activation='relu', #relu\n",
    "                    solver='adam', \n",
    "                    max_iter=30000, #300000\n",
    "                    batch_size='auto',\n",
    "                    learning_rate_init=0.001,\n",
    "                    # Early stopping kinda does CV too https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "                    early_stopping=True,\n",
    "                    shuffle=True,\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    alpha=0.0001, # L2 loss strenght\n",
    "                    beta_1=0.9, # 0.9 org Exponential decay rate for estimates of first moment vector in adam\n",
    "                    beta_2=0.999, # 0.999 org Exponential decay rate for estimates of second moment vector in adam\n",
    "                    epsilon=1e-8 # 1e-8 org Value for numerical stability in adam.\n",
    "                    )\n",
    "\n",
    "parameters = {\n",
    "    \"classifier__solver\": [\"adam\", \"sgd\"],\n",
    "    \"classifier__activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "    \"classifier__learning_rate_init\": [0.0001, 0.001, 0.01, 0.005],\n",
    "    \"classifier__hidden_layer_sizes\": [[10,10], [100,10], [50,100,50], [100], [10]],\n",
    "    \"classifier__beta_1\": [round(i*0.001, 3) for i in range(90, 95)],\n",
    "    \"classifier__beta_2\": [round(i*0.001, 4) for i in range(985, 999, 3)]\n",
    "}\n",
    "print(parameters)\n",
    "\n",
    "mlp_cls = halving_random_search(mlp, SCALER, parameters, random_state=RANDOM_STATE)\n",
    "\n",
    "mlp_cls.fit(X_train, y_train)\n",
    "\n",
    "print(mlp_cls.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "mlp_avg, mlp_cm = validate(mlp_cls, X_val, y_val)\n",
    "writer.add_scalars(\"mlp\", mlp_avg)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
