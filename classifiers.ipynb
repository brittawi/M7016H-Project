{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptive_statistics import DiabetesDataBase\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from utils import grid_search, halving_random_search, validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALER = StandardScaler()\n",
    "SCALER = RobustScaler() # No difference to standard scaler for models I test except mlp\n",
    "#SCALER = QuantileTransformer(n_quantiles=334) # Slightly better result for regression and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "time = datetime.now().strftime(\"%Y%B%d_%H_%M\")\n",
    "print(time)\n",
    "log_folder = \"logs/\"+time\n",
    "writer = SummaryWriter(log_dir=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"diabetes.csv\"\n",
    "ddb = DiabetesDataBase(csv_path)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = ddb.splitData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1, 31))\n",
    "distance_metrics = ['euclidean','manhattan']\n",
    "weight_options = ['uniform','distance']\n",
    "param_grid = dict(classifier__n_neighbors=k_range,\n",
    "                  classifier__metric=distance_metrics,\n",
    "                  classifier__weights=weight_options)\n",
    "knn_clf = KNeighborsClassifier()\n",
    "pipe = Pipeline(steps=[(\"classifier\", knn_clf)])\n",
    "clf_GS = GridSearchCV(pipe, param_grid, return_train_score=True)\n",
    "# training is done on the train set!!\n",
    "clf_GS.fit(X_train, y_train)\n",
    "\n",
    "# print best values for each parameter\n",
    "for key in param_grid.keys():\n",
    "    print(f'Best {key}:', clf_GS.best_estimator_.get_params()[f'{key}'])\n",
    "print(clf_GS.best_estimator_.get_params()['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best estimator to predict on the validation set\n",
    "y_pred = clf_GS.best_estimator_.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average=None)\n",
    "recall = recall_score(y_val, y_pred, average=None)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# show confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[True,False])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "\n",
    "param_grid = dict(classifier__criterion=criterion,\n",
    "                classifier__max_depth=max_depth)\n",
    "print(param_grid)\n",
    "decision_tree_clf = DecisionTreeClassifier()\n",
    "pipe = Pipeline(steps=[(\"classifier\", decision_tree_clf)])\n",
    "clf_GS = GridSearchCV(pipe, param_grid, return_train_score=True)\n",
    "# training is done on the train set!!\n",
    "clf_GS.fit(X_train, y_train)\n",
    "\n",
    "# print best values for each parameter\n",
    "for key in param_grid.keys():\n",
    "    print(f'Best {key}:', clf_GS.best_estimator_.get_params()[f'{key}'])\n",
    "print(clf_GS.best_estimator_.get_params()['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best estimator to predict on the validation set\n",
    "y_pred = clf_GS.best_estimator_.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average=None)\n",
    "recall = recall_score(y_val, y_pred, average=None)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# show confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[True,False])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "\n",
    "parameters = {\n",
    "    \"classifier__penalty\": [None, \"l2\"]\n",
    "}\n",
    "\n",
    "reg_cls = grid_search(log_reg, SCALER, parameters)\n",
    "\n",
    "reg_cls.fit(X_train, y_train)\n",
    "\n",
    "print(reg_cls.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "log_reg_avg, log_reg_cm = validate(reg_cls, X_val, y_val)\n",
    "writer.add_scalars(\"log_reg\", log_reg_avg)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "parameters = {\n",
    "    \"classifier__C\": [round(i*0.2, 1) for i in range(1, 21)],\n",
    "    \"classifier__gamma\": [round(i*0.1, 1) for i in range(1, 21)],\n",
    "    \"classifier__kernel\": [\"linear\", \"rbf\", \"sigmoid\"]\n",
    "}\n",
    "print(parameters)\n",
    "\n",
    "\n",
    "svm_cls = grid_search(svm, SCALER, parameters)\n",
    "svm_cls.fit(X_train, y_train)\n",
    "print(svm_cls.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "svm_avg, svm_cm = validate(svm_cls, X_val, y_val)\n",
    "writer.add_scalars(\"svm\", svm_avg)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(activation='relu', #relu\n",
    "                    solver='adam', \n",
    "                    max_iter=30000, #300000\n",
    "                    batch_size='auto',\n",
    "                    learning_rate_init=0.001,\n",
    "                    # Early stopping kinda does CV too https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "                    # But got worse results\n",
    "                    early_stopping=True, # False\n",
    "                    shuffle=True,\n",
    "                    random_state=17,\n",
    "                    alpha=0.0001, # L2 loss strenght\n",
    "                    beta_1=0.9, # 0.9 org Exponential decay rate for estimates of first moment vector in adam\n",
    "                    beta_2=0.999, # 0.999 org Exponential decay rate for estimates of second moment vector in adam\n",
    "                    epsilon=1e-8 # 1e-8 org Value for numerical stability in adam.\n",
    "                    )\n",
    "\n",
    "\"\"\"\n",
    "# These found acc: 77 f1 74,76\n",
    "# MLPClassifier(batch_size=64, beta_1=0.93, beta_2=0.99, epsilon=1.2e-08, max_iter=30000, random_state=17, solver='sgd')\n",
    "parameters = {\n",
    "    \"classifier__solver\": [\"adam\", \"sgd\"],\n",
    "    \"classifier__batch_size\": [4, 16, 32, 64],\n",
    "    \"classifier__activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "    \"classifier__learning_rate_init\": [0.0001, 0.001, 0.01, 0.005],\n",
    "}\n",
    "\n",
    "# These found acc 75, f1 72, 75\n",
    "# MLPClassifier(beta_1=0.09, beta_2=0.988, epsilon=1.2e-08, hidden_layer_sizes=[10], max_iter=30000, random_state=17)\n",
    "parameters = {\n",
    "    \"classifier__hidden_layer_sizes\": [[10,10], [100,10], [50,100,50], [100], [10]],\n",
    "    \"classifier__beta_1\": [round(i*0.001, 3) for i in range(90, 95)],\n",
    "    \"classifier__beta_2\": [round(i*0.001, 4) for i in range(985, 999, 3)]\n",
    "}\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    \"classifier__solver\": [\"adam\", \"sgd\"],\n",
    "    #\"classifier__batch_size\": [4, 16, 32, 64],\n",
    "    \"classifier__activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "    \"classifier__learning_rate_init\": [0.0001, 0.001, 0.01, 0.005],\n",
    "    \"classifier__hidden_layer_sizes\": [[10,10], [100,10], [50,100,50], [100], [10]],\n",
    "    #\"classifier__beta_1\": [round(i*0.001, 3) for i in range(90, 95)],\n",
    "    #\"classifier__beta_2\": [round(i*0.001, 4) for i in range(985, 999, 3)]\n",
    "}\n",
    "print(parameters)\n",
    "for i in range(5):\n",
    "    mlp_cls = grid_search(mlp, SCALER, parameters)\n",
    "\n",
    "    mlp_cls.fit(X_train, y_train)\n",
    "\n",
    "    print(mlp_cls.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "mlp_avg, mlp_cm = validate(mlp_cls, X_val, y_val)\n",
    "writer.add_scalars(\"mlp\", mlp_avg)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
